# 1、HDFS概述

## 1.1、HDFS的定义

* HDFS（Hadoop Distributed File System），它是一个文件系统；其次，它是分布式的。
* HDFS的使用场景：适合一次写入，多次读出的场景。

## 1.2、HDFS的优点

1. 高容错性

   ![](https://qqs-images.oss-cn-shenzhen.aliyuncs.com/%E9%AB%98%E5%AE%B9%E9%94%99%E6%80%A7.svg)

2. 适合处理大数据

   1. 数据规模：GB、TB、甚至PB级别的数据。
   2. 文件规模：百万规模以上的文件数量。

3. 可构建在廉价机器上，通过多副本机制，提高可用性。

## 1.3、HDFS的缺点

1. 不适合低延迟数据访问，比如毫秒级无法做到。

2. 无法高效的对大量小文件进行存储。

   * 存储大量小文件会占用NameNode大量的内存来存储文件目录和块信息。
   * 小文件存储的寻址时间会超过读取文件内容的时间，违背了HDFS的设计理念。

3. 不支持并发写入、文件随机修改。

   * 同时只能有一个文件写入，不能多线程多文件同时写入。

   * 只支持数据的追加，不支持文件的随机修改。

     ![不支持文件修改](https://qqs-images.oss-cn-shenzhen.aliyuncs.com/%E4%B8%8D%E6%94%AF%E6%8C%81%E6%96%87%E4%BB%B6%E4%BF%AE%E6%94%B9.svg)

## 1.4、HDFS的组成架构

![HDFS组成架构](https://qqs-images.oss-cn-shenzhen.aliyuncs.com/HDFS%E7%BB%84%E6%88%90%E6%9E%B6%E6%9E%84.svg)

1. NameNode（nn）：Master，HDFS的管理者。
   1. 管理HDFS的名称空间。
   2. 配置副本策略。
   3. 管理数据块（Block）映射信息。
   4. 处理客户端读写请求。
2. DataNode：Slave，NameNode下达命令，DataNode执行实际的操作。
   1. 存储实际的数据块。
   2. 执行数据块的读/写操作。
3. Client：客户端。
   1. 文件切分；文件上传HDFS时，Client将文件切分成一个或多个Block，然后进行上传。
   2. 与NameNode交互，获取文件的位置信息。
   3. 与DataNode交互，读取或写入数据。
   4. 通过命令管理HDFS，比如NameNode格式化。
   5. Client通过命令访问HDFS，比如对HDFS增删改查操作等。
4. Secondary NameNode：并非NameNode的热备；与NameNode挂掉时，它并不能马上替换NameNode并提供服务。
   1. 辅助NameNode，分担其工作量，比如定期合并Fsimages和Edits，并推送给NameNode。
   2. 在紧急情况下，可辅助恢复NameNode。

## 1.5、文件块大小

* HDFS的文件在物理上是分块存储（Block），块的大小可以通过配置参数（dfis.blcksize）来设置，默认大小在Hadoop2.x/3.x版本中是128M，1.x版本中是64M。
* 如果寻址时间约为10ms，即查找目标Block的时间为10ms。
* 寻址时间为传输时间的1%时为最佳状态。因此，传输时间=10ms /0.01=1000ms=1s。
* 目前磁盘的传输速率普遍为100Mb/s。

* **思考：为什么块的大小不能设置太大或者太小。**

  * 如果块设置太小，会增加寻址时间。

  * 如果块设置太大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间（寻址时间）。导致在处理块数据时，会非常慢。
  * **总结：HDFS块的大小设置主要取决于磁盘传输速率**。

# 2、HDFS的命令操作

## 2.1、基本语法

* hadoop fs 或 hdfs dfs 两个都是完成相同的。

## 2.2、常用大全

* 通过bin/hadoop fs命令查看所有命令。

| 序号 | 命令                    | 作用                                        |
| :--- | :---------------------- | :------------------------------------------ |
| 1    | -help                   | 查看命令帮助信息，如参数等                  |
| 2    | -ls                     | 显示目录信息                                |
| 3    | -mkdir                  | 在HDFS上创建目录                            |
| 4    | -moveFromLocal          | 从本地剪切粘贴到HDFS                        |
| 5    | -appendToFile           | 追加一个文件到已经存在的文件末尾            |
| 6    | -cat                    | 显示文件内容                                |
| 7    | -chgrp 、-chmod、-chown | Linux文件系统中的用法一样，修改文件所属权限 |
| 8    | -copyFromLocal          | 从本地文件系统中拷贝文件到HDFS路径去        |
| 9    | -copyToLocal            | 从HDFS拷贝到本地                            |
| 10   | -cp                     | 从HDFS的一个路径拷贝到HDFS的另一个路径      |
| 11   | -mv                     | 在HDFS目录中移动文件                        |
| 12   | -get                    | 等同于copyToLocal，就是从HDFS下载文件到本地 |
| 13   | -getmerge               | 合并下载多个文件                            |
| 14   | -put                    | 等同于copyFromLocal                         |
| 15   | -tail                   | 显示一个文件的末尾                          |
| 16   | -rm                     | 删除文件或文件夹                            |
| 17   | -rmdir                  | 删除空目录                                  |
| 18   | -du                     | 统计文件夹的大小信息                        |
| 19   | -setrep                 | 设置HDFS中文件的副本数量                    |
| 20   | - expunge               | 清空HDFS垃圾桶                              |

## 2.3、上传

* -moveFromLocal：从本地剪切粘贴到HDFS

  ```shell
  # 准备文件
  echo "shuguo" >> shuguo.txt
  # 剪切
  hadoop fs  -moveFromLocal  ./shuguo.txt  /sanguo
  ```

* -copyFromLocal：本地文件系统中拷贝文件到HDFS

  ```shell
  # 准备文件
  echo "weiguo" >> weiguo.txt
  # 拷贝
  hadoop fs  -copyFromLocal  ./weiguo.txt  /sanguo
  ```

* -put：等同于copyFromLocal，一般使用put

  ```shell
  # 准备文件
  echo "wuguo" >> wuguo.txt
  # put
  hadoop fs -put ./wuguo.txt /sanguo
  ```

* -appendToFile：追加一个文件到已存在文件末尾

  ```shell
  # 准备文件
  echo "liubei" >> liubei.txt
  # 追加
  hadoop fs -appendToFile liubei.txt /sanguo/shuguo.txt
  ```

## 2.4、下载

* -copyToLocal：从HDFS拷贝到本地

  ```shell
  hadoop fs -copyToLocal /sanguo/shuguo.txt ./
  ```

* -get：等同于copyToLocal，一般使用put

  ```shell
  hadoop fs -get /sanguo/shuguo.txt ./shuguo2.txt
  ```

## 2.5、操作HDFS

* -ls：查看目录信息

  ```shell
  hadoop fs -ls /sanguo
  ```

* -cat：查看文件内容

  ```shell
  hadoop fs -cat /sanguo/shuguo.txt
  ```

* -chgrp、-chmod、-chown：与linux文件系统中的用法一样，修改文件所属权限

  ```shell
  # 修改文件权限
  hadoop fs -chmod 666 /sanguo/shuguo.txt
  
  # 修改文件
  hadoop fs -chown qqs:qqs /sanguo/shuguo.txt
  ```

  > 只有超级用户和属于组的文件所有者才能变更文件关联组。非超级用户如需要设置关联组可能需要使用 [chgrp](https://www.runoob.com/linux/linux-comm-chgrp.html) 命令。

* -mkdir：创建目录

  ```shell
  hadoop fs -mkdir /jinguo
  ```

* -cp：拷贝文件

  ```shell
  hadoop fs -cp /sanguo/shuguo.txt /jinguo
  ```

* -mv：移动文件

  ```shell
  hadoop fs -mv /sanguo/wuguo.txt /jinguo
  ```

* -tail：从文件末尾查看1kb的数据

  ```shell
  hadoop fs -tail /jinguo/shuguo.txt 
  ```

* -rm：删除文件或文件夹

  ```shell
  hadoop fs -rm /sanguo/shuguo.txt
  ```

* -rm -r：递归删除目录及目录里的所有内容

  ```shell
  hadoop fs -rm -r sanguo
  ```

* -du：统计文件夹的大小信息

  ```shell
  hadoop fs -du -s -h /jinguo
  
  hadoop fs -du -h /jinguo
  ```

​		![image-20230107224250970](https://qqs-images.oss-cn-shenzhen.aliyuncs.com/image-20230107224250970.png)

​		说明：18表示文件大小；54 = 18 * 3（副本数）。

* -setrep：设置文件的副本数量

  ```shell
  hadoop fs -setrep 5 /sanguo/shuguo.txt
  ```

  > 注意：设置的副本数只是记录在NameNode的元数据中，是否真会有这么多副本，需要看DataNode的数量。

# 3、HDFS的API操作

## 3.1、客户端环境准备

* 下载依赖文件：https://github.com/cdarlint/winutils

* 配置环境变量

  ![image-20230107234250679](https://qqs-images.oss-cn-shenzhen.aliyuncs.com/image-20230107234250679.png)

![](https://qqs-images.oss-cn-shenzhen.aliyuncs.com/image-20230107234358478.png)

* 编写测试类

  ```java
  public class HdfsClient {
      
      public static final String DEV_INPUT = "D:/learn/大数据/big-data/hadoop/input/";
      public static final String DEV_OUTPUT = "D:/learn/大数据/big-data/hadoop/output/";
      
      @Test
      public void mkdir() throws Exception {
          Configuration config = new Configuration();
          // hp: 指定访问HDFS的用户,防止权限报错
          FileSystem fs = FileSystem.get(new URI("hdfs://hp-node1:8020"), config, "hp");
          fs.mkdirs(new Path("/xiyou"));
          fs.close();
      }
  }
  ```

## 3.2、上传（测试参数优先级）

* code

```java
@Test
public void copyFromLocalFile() throws Exception {
    Configuration config = new Configuration();
    config.set("dfs.replication", "2");
    FileSystem fs = FileSystem.get(new URI("hdfs://hp-node1:8020"), config, "hp");
    fs.copyFromLocalFile(new Path(DEV_INPUT + "sunwukong.txt"), new Path("/xiyou"));
    fs.close();
}
```

* 将hdfs-site.xml拷贝到项目的resources资源目录下。

  ```xml
  <?xml version="1.0" encoding="UTF-8"?>
  <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
  <configuration>
      <property>
      <name>dfs.replication</name>
       <value>1</value>
      </property>
  </configuration>
  ```

* **参数优先级：**

  1. 客户端代码中设置的参数值。
  2. ClassPath下的用户自定义配置文件。
  3. 服务器的自定义配置（xxx-site.xml）。
  4. 服务器的默认配置（xxx-default.xml）。

## 3.3、下载

```java
@Test
public void copyToLocalFile() throws Exception {
    Configuration config = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hp-node1:8020"), config, "hp");
    fs.copyToLocalFile(new Path("/xiyou/sunwukong.txt"), new Path(DEV_OUTPUT + "sunwukong.txt"));
    fs.close();
}
```

## 3.4、删除文件和目录

```java
@Test
public void delete() throws Exception {
    Configuration config = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hp-node1:8020"), config, "hp");
    fs.delete(new Path("/xiyou"), true);
    fs.close();
}
```

## 3.5、文件详情查看

```java
@Test
public void listFileInfo() throws Exception {
    Configuration config = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hp-node1:8020"), config, "hp");
    // 文件详情
    RemoteIterator<LocatedFileStatus> files = fs.listFiles(new Path("/"), true);
    while (files.hasNext()) {
        LocatedFileStatus fileStatus = files.next();
        System.out.println("======" + fileStatus.getPath() + "======");
        // 权限
        System.out.println(fileStatus.getPermission());
        // 所有者
        System.out.println(fileStatus.getOwner());
        // 组
        System.out.println(fileStatus.getGroup());
        // 文件大小
        System.out.println(fileStatus.getLen());
        // 最后一次修改时间
        System.out.println(fileStatus.getModificationTime());
        // 副本数
        System.out.println(fileStatus.getReplication());
        // 块大小
        System.out.println(fileStatus.getBlockSize());
        // 文件名
        System.out.println(fileStatus.getPath().getName());
        // 块信息
        BlockLocation[] blockLocations = fileStatus.getBlockLocations();
        System.out.println(Arrays.toString(blockLocations));
    }
    fs.close();
}
```

## 3.6、文件和文件夹判断

```java
@Test
public void fileJudge() throws Exception {
    Configuration config = new Configuration();
    FileSystem fs = FileSystem.get(new URI("hdfs://hp-node1:8020"), config, "hp");
    FileStatus[] fileStatuses = fs.listStatus(new Path("/"));
    for (FileStatus fileStatus : fileStatuses) {
        if (fileStatus.isFile()) {
            System.out.println("f:" + fileStatus.getPath().getName());
        }else {
            System.out.println("d:" + fileStatus.getPath().getName());
        }
    }
    fs.close();
}
```

# 4、HDFS的读写流程

## 4.1、HDFS的数据写流程

![HDFS写数据流程](https://qqs-images.oss-cn-shenzhen.aliyuncs.com/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.svg)

## 4.2、节点距离计算（网络拓扑）

## 4.3、副本存储节点选择（机架感知）

## 4.4、HDFS的数据读流程

# 5、NameNode

## 5.1、NN和2NN的工作机制

## 5.2、Fsimage和Edits解析

## 5.3、CheckPoint检查点

# 6、DataNode

## 6.1、DataNode的工作机制

## 6.2、数据完整性

## 6.3、掉线时限参数设置